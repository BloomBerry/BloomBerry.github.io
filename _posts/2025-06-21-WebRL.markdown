---
title: "# WebRL: Training LLM Web Agents Via Self-Evolving Online Curriculum Reinforcement Learning"
---



# [WebAgent] WebRL: Training LLM Web Agents Via Self-Evolving Online Curriculum Reinforcement Learning

- paper: https://arxiv.org/pdf/2411.02337
- github: https://github.com/THUDM/WebRL
- ICLR 2025 accepted (인용수: 9회, '25-06-24 기준)
- downstream task: webarena

# 1. Motivation

- 기존의 LLM기반 Web agents는 대부분 조작된 prompt와 LLM API를 활용하여 web page를 이해하고 조작했는데, 이는 비용이 비싸고, 시간이 오래걸린다.

- *Sparsity & cost of feedback signals*: open-source LLM은 pretraining/post-training 시에 decision-centric data의 부족으로 전문적인 web agent 수준에 성능이 미치지 못한다.

- *Insufficienty of training tasks*: 최근에 Imitation learning (SFT) 기반으로 연구가 시도 되었으나, web interaction의 online nature를 반영하기에는 데이터의 양이 불충분하고, continual improvement가 힘들다. 

- *Policy Distribution drift in online learning*: 학습 중에 catastrophic forgetting이 생길 수 있다.

  $\to$ 이러한 문제를 해결하기 위해 WebRL framework를 제안해보자!

# 2. Contribution

- 새로운 자가 발전하는 LLM기반 web agent로서, Curriculum RL framework인 WebRL을 제안함
  - WebArena 환경에서 최초로 RL을 도입했음
  - ORM (Output-supervised Reward Model) 적용
- 3가지 핵심 challenge를 풀고자 했음
  - 학습 task의 부족 $\to$ Self-evolving curriculum & adaptive learning 전략
  - Feedback signal의 sparsity $\to$ confidence 기반의 threshold 조절
  - Distribution shift $\to$ KL Divergence + Experience replay buffer
- WebArena-benchmark에서 SOTA를 달성함 
  - Chatgpt보다 160% 성능이 우수함

# 3. WebRL

- Framework

  ![](../images/2025-06-24/image-20250624211137522.png)

  - Agent는 지속적으로 Environment와 상호작용하면서 실시간 trajectory data를 취합함.
  - 해당 상호작용은 Agent의 현재 상태에서의 proficiency를 고려하여  Self-evolving curriculum 전략에 가이드를 받아 진행됨
  - ORM (Outcome-supervised Reward Model) 역시 학습하여 task의 success를 평가함
  - KL-Constraint + Relpay buffer를 두어 polciy update algorithm을 조절하여 극단적인 policy shift를 방지함

- Algorithm

  ![](../images/2025-06-24/image-20250624212141849.png)

- ORM Training

  - LLM 모델에게 최종 web page HTML + agent의 action history를 주고, 해당 task를 success $\to$ 1 else $\to$ 0을 주도록 학습함

    ![](../images/2025-06-24/image-20250624215805305.png)

  - WebRL은 학습된 ORM 모델을 활용하여 Reward signal을 도출함

## 3.1 Self-Evolving New Instruction for Curriculum Learning

- 매 Phase마다 (Curriculum Learning으로) task를 생성하면서 학습 task부족 이슈를 해결함

- 학습이 진행될수록, task의 난이도는 상승함

- 2 step approach를 사용

  - generation step: in-breadth evolving approach를 사용하여 이전 interaction phase들에서 fail한 task를 seed로 활용하여 새로운 instruction을 생성함

  - filtering step: 학습된 critic 모델을 활용하여 생성된 instruction중, 해당 환경과 align되어 있고, 요구되는 어려움 난이도가 feasible한 것만 남기게 함

    - critic score가 0.05 ~0.75 사이에 존재해야 함.

    - 수동으로 생성된 task를 분별하여 WebArena에서 수행하지 못하는 task를 걸러넴

      $\to$ GPT를 써서 해당 infeasible task를 제외시킴

      ![](../images/2025-06-24/image-20250624215845162.png)

## 3.2 Reinforcement Learning for LLMs in Online Web Environments

- 정책 모델 목표함수

  ![](../images/2025-06-24/image-20250625142120613.png)

  - $max_{\pi_\theta}$: 정책 $\pi_\theta$의 매개변수 $\theta$에 대해 수식의 값을 최대화합니다.
  - $\mathbb{E}_{I \sim \rho(I),a_t \sim \pi_\theta(\cdot|s_t)}$: 목표 함수의 값은 현재 단계의 작업 분포 $\rho(I)$에서 추출된 instruction I와, 현재 정책 $\pi_\theta$에 따라 상태 $s_t$에서 추출된 행동 $a_t$에 대한 기대값. 즉, 다양한 작업 환경에서 현재 정책을 따랐을 때의 평균적인 결과를 의미.
  - $\sum_{t=0}^T$: 시간 단계 \(t=0\)부터 종료 시간 \(T\)까지의 합산. 각 시간 단계에서 얻는 보상과 정책 관련 항을 누적.
  - $r(s_t, a_t, I)$: 시간 t에 상태 $s_t$에서 행동 $a_t$를 취하고 instruction I를 따랐을 때 얻는 보상. 웹 에이전트 태스크에서는 일반적으로 작업 성공 시에만 마지막 단계에서 1의 보상을 받고, 그 외에는 0의 보상을 받는 희소(sparse)한 형태임.
  - $\beta \log \pi_{\text{ref}}(a_t|s_t, I)+\beta\mathcal{H}(\pi_\theta)$: 이 항은 현재 정책 $\pi_\theta$가 이전 단계의 정책 $\pi_{\text{ref}}$에서 크게 벗어나지 않도록 제약하는 역할 수행. $\beta$는 이 제약의 강도를 조절하는 계수. 이 값을 최대화하는 것은 $\pi_\theta$와 $\pi_{\text{ref}}$ 간의 KL-divergence를 최소화하는 것과 관련이 있음. 이는 온라인 학습 중 발생하는 정책 분포 드리프트(policy distribution drift)를 완화하여 안정적인 학습에 기여함.

- 가치 모델 (V) 손실 함수

  - 목적: 주어진 action, instruction, state를 보고 reward를 예측하는 모델

  ![](../images/2025-06-24/image-20250625165455026.png)

  - $L(V)$: 가치 네트워크 V의 손실 함수.
  - $E_{\nu}[\dots]$: 데이터 분포 \($\nu$)에 대한 기대값. 데이터 $\nu$는 학습 과정에서 수집된 궤적들로 구성.
  - $r(s_T, a_T, I)$: 마지막 상태 $s_T$에서 행동 $a_T$를 통해 얻은 보상. 여기서는 작업 성공 시 1, 실패 시 0의 이진 보상.
  - $V(s, a, I)$: 가치 네트워크가 예측하는 특정 상태 s, 행동 a, 명령어 I에 대한 가치. 이 손실 함수에서는 주로 최종 상태에서의 성공 확률을 예측하는 데 사용.
  - $\log V(s, a, I)$: 가치 네트워크가 성공(보상 1)을 예측할 확률의 로그값.
  - $\log(1 - V(s, a, I))$: 가치 네트워크가 실패(보상 0)를 예측할 확률의 로그값입니다.


# 4. Experiments

# 5. Related Works