---
title: "[LLM] DeepSeek-V3 Technical Report"

---
# [LLM] DeepSeek-V3 Technical Report

- paper: https://arxiv.org/pdf/2412.19437
- github: https://github.com/deepseek-ai/DeepSeek-V3 (Training code X)
- archived (ì¸ìš©ìˆ˜: 0íšŒ, '25-02-01 ê¸°ì¤€)
- downstream task: VQA

# 1. Introduction

- Opensource ëª¨ë¸ì˜ ì„±ëŠ¥ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³ ì **671B (37B-activated)**ì˜ **MoE**ê¸°ë°˜ LLM ëª¨ë¸ì¸ **Deepseek-V3**ë¥¼ ê°œë°œí•˜ê²Œ ë˜ì—ˆìŒ

- ì£¼ìš” íŠ¹ì§•

  - í•™ìŠµì˜ íš¨ìœ¨ì„± 

    - Mixture-of-Experts (MoE)ê¸°ë°˜ìœ¼ë¡œ ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜ (671B)ëŒ€ë¹„ ì¼ë¶€ íŒŒë¼ë¯¸í„° (37B)ë§Œ activateë¨ìœ¼ë¡œì¨, í¬ê¸° ëŒ€ë¹„ í•™ìŠµì´ íš¨ìœ¨ì ì„ (DeepSeek-V2ì™€ ë™ì¼)

    - **FP8 mixed precision training**ì„ ë„ì…í•¨

    - **Efficient cross-node all-to-all communication kernel**ì„ ë„ì…í•¨ $\to$ InfiniBand (IB) & NVLinkë¥¼ 10ë¶„ í™œìš©

      - Pre-training

        - tokens: 14.8T high-quality diverse tokens
        - cost:  **2.788M @ H800 GPU hours** (H800x2,048ë¡œ 3.7ì¼)
        - max length: 32K

      - Post-training (SFT + RL / Distillation (@R1 series))

        - cost: **0.1M @ H800 GPU hours** 
        - max length: 128K

        ![](../images/2025-02-01/image-20250131103638568.png)$\to$ ë¯¸êµ­ BigTechëŒ€ë¹„ 10%

  - ì¶”ë¡ ì˜ íš¨ìœ¨ì„± (DeepSeek-V2ì™€ ë™ì¼)

    - Multi-head Latent Attention (MLA) ê¸°ë°˜ 
      - LoRA (Low-Rank Adaptaion)ì²˜ëŸ¼ ì¤‘ê°„ì— Query ì™€ Key-Valueì˜ feature sizeë¥¼ ì¤„ì˜€ë‹¤ê°€, ëŠ˜ë ¤ì£¼ëŠ” Bottleneckêµ¬ì¡° ë„ì… $\to$ Inferenceí•  ë•Œ, KeyValue cacheì˜ ë©”ëª¨ë¦¬ë¥¼ íšê¸°ì ìœ¼ë¡œ ì¤„ì—¬ì¤Œ

  - **Auxiliary Loss-free** ì „ëµìœ¼ë¡œ **load-balancing**ì„ í•¨ìœ¼ë¡œì¨, domainë³„ Expert (í˜¹ì€ specialist)ì˜ ì—­í• ì„ ê°€ì¤‘í•¨

    - Auxiliary loss for load-balance?
      - LLMì˜ íŠ¹ì • MoE headë§Œ activationë˜ëŠ” í˜„ìƒ (load imbalancing)ì„ í•´ê²°í•˜ê¸° ìœ„í•´ auxiliary lossë¥¼ ë¶€ì—¬í•˜ê³  ìˆì—ˆìŒ (ê°•ì œë¡œ ì—¬ëŸ¬ headê°€ activateë˜ë„ë¡.)
      - í•˜ì§€ë§Œ, ì´ëŸ° auxiliary lossëŠ” ì„±ëŠ¥ í•˜ë½ì„ ì•¼ê¸°í•  ìˆ˜ ìˆìŒ

  - Multi-Token Prediction (MTP) trainingì„ ë„ì…í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ

- ì£¼ìš” benchmarkì—ì„œ SOTA

  ![](../images/2025-02-01/image-20250131104005280.png)

  - Knowledge
    - Education benchmark:  MMLU, MMLU-Pro, GPQA
    - Factuality benchmark: SimpleQA, Chinese SimpleQA
  - Code & Match Reasoning
    - Math related benchmark: MATH-500
    - Coding benchmark: LiveCodeBench

# 2. Architecture

## 2.1 Basic Architecture

- DeepSeek-V2 ê·¸ëŒ€ë¡œ ì°¨ìš© (MoE + MLA)![](../images/2025-02-01/image-20250131105034244.png)

- Multi-head Latent Attention (MLA)

  - í•µì‹¬: Low-rank (keyValue joint) compression $d_c (<< d_hn_h), d'_c(<< d_hn_h)$

    ![](../images/2025-02-01/image-20250131150852171.png)

    ![](../images/2025-02-01/image-20250131150917224.png)

    - $d_h$: head dimension
    - $n_h$: multi-head ê°¯ìˆ˜
    - $d_c$: key,value compression dimension
    - $d'_c$: query compression dimension


  ![](../images/2025-02-01/image-20250131105142046.png)

  - **h**$_t \in \mathbb{R}^d$ : të²ˆì§¸ input hidden embedding

  - **c**$_t^{KV} \in \mathbb{R}^{d_c}$ : down-projection (*W*$^{DK} \in \mathbb{R}^{d_c \times d}$) í†µê³¼í•œ Key, Value embedding

  - **k**$_t^C \in \mathbb{R}^{d_hn_h}$ : up-projection (*W*$^{UK} \in \mathbb{R}^{d_hn_h \times d_c}$) í†µê³¼í•œ key embedding

  - **v**$_t^C \in \mathbb{R}^{d_hn_h}$ : up-projection (*W*$^{UV} \in \mathbb{R}^{d_hn_h \times d_c}$) í†µê³¼í•œ value embedding

  - **k**$_t^R \in \mathbb{R}^{d_h^R}$ : Rotary projection (*W*$^{KR} \in \mathbb{R}^{d_h^R \times d}$) í†µê³¼í•œ Rotation Position embedding

    $\to$ íŒŒë€ìƒ‰ì€ inferenceë•Œ key, value cachingë¨. Memory efficientí•¨

    ![](../images/2025-02-01/image-20250131143206168.png)

    - **h**$_t \in \mathbb{R}^d$ : të²ˆì§¸ input hidden embedding
    - **c**$_t^{Q} \in \mathbb{R}^{d'_c}$ : down-projection (*W*$^{DQ} \in \mathbb{R}^{d'_c \times d}$) í†µê³¼í•œ Key, Value embedding
    - **q**$_t^C \in \mathbb{R}^{d_hn_h}$ : up-projection (*W*$^{UQ} \in \mathbb{R}^{d_hn_h \times d'_c}$) í†µê³¼í•œ value embedding
    - **q**$_t^R \in \mathbb{R}^{d_h^Rn_h}$ : up-projection (*W*$^{UQ} \in \mathbb{R}^{d_h^Rn_h \times d'_C}$) í†µê³¼í•œ Rotation Position embedding

- Multi-head Latent Attention

  ![](../images/2025-02-01/image-20250131143907884.png)

  - *W*$^O$: Output projection matrix

## 2.2 DeepseekMoE with Auxiliary-Loss-Free Load Balancing

- DeepSeekMoE vs. MoE

  - DeepSeekV2ì™€ ì°¨ë³„ì : activation function softmax $\to$ sigmoidë¡œ ë°”ê¾¸ê³ , normalizationì„ ìˆ˜í–‰

  - Shared expertì™€ fine-grained isolated (routed) expertë¡œ expertë¥¼ ì´ë¶„í™”í•¨

    ![](../images/2025-02-01/image-20250131145717301.png)

    - **h**'$_t$: të²ˆì§¸ tokenì˜ DeepSeekMoE output
    - **u**$_t$: të²ˆì§¸ tokenì˜ DeepSeekMoE input
    - $FFN_i^{(s)}$: ië²ˆì§¸ expertì˜ shared Feed-Forward Network
    - $FFN_i^{(r)}$: ië²ˆì§¸ expertì˜ routed Feed-Forward Network
    - $N_s$: shared expertì˜ ê°¯ìˆ˜
    - $N_r$: routed expertì˜ ê°¯ìˆ˜
    - **e**$_i$: ië²ˆì§¸ routed expertì˜ centroid vector (?)
    - $g_{i,t}$: të²ˆì§¸ tokenì˜ ië²ˆì§¸ routed expertì˜ **token-to-expert affinity score**
    - $K_r$: TopKì˜ Affinity score ê°¯ìˆ˜

- Auxiliary-Loss-Free Load Balancing

  - layerë³„ MLAì—ì„œ affinity scoreë¥¼ ê³„ì‚°í•  ë•Œ, biasë¥¼ ì¶”ê°€í•´ì¤Œìœ¼ë¡œì¨ í•´ê²°

    ![](../images/2025-02-01/image-20250131152405918.png)

    - $b_j$: jë²ˆì§¸ TopKì— í¬í•¨ëœ routed expertì˜ bias. iteration ëŒë•Œë§ˆë‹¤ $\gamma$ ë§Œí¼ ì¤„ì–´ë“¦. (hyper-parameter)

      $\to$ë‹¨, routingí• ë•Œë§Œ $g'_{i,t}$ê°€ ì‚¬ìš©ë˜ë©°, ì‹¤ì œ affinity scoreëŠ” originalí•œ ê°’ì„ í™œìš©.

- Complementary Sequence-wise Balance Loss

  - ëª©ì : extreme imbalanceëœ expert selectionì„ ë°©ì§€í•˜ê³ ì, balance lossë¥¼ ë„£ê¸´ í•¨.

    ![](../images/2025-02-01/image-20250131153436604.png)

    $\to$ affinity score ($s_i$)ê°€ 0ì— ê°€ê¹Œì›Œì§€ëŠ” ëª©ì  (ì¦‰, ê· ì¼í•´ì§€ëŠ” ì—­í• )

- Node-limited routing

  - ëª©ì : *M*ê°œì˜ node (ex. H800x8ëŒ€ server)ë³„ë¡œ Top $\frac{K_r}{M}$ê°œì˜ affinity scoreë¥¼ê³„ì‚°í•˜ì—¬ í•©ì¹¨ìœ¼ë¡œì¨, communication costë¥¼ ì¤„ì´ê³ ì í•¨

- No token Dropping

  - ê¸°ì¡´ì— MoEëŠ” (DropOut ê°™ì´?) imbalance tokenì„ dropí•œ ê²ƒ ê°™ìŒ. 
  - í•˜ì§€ë§Œ DeepSeekMoEëŠ” load-balancingì´ ì˜ë˜ì–´, 1ê°œì˜ tokenë„ dropí•˜ì§€ ì•Šì•˜ë‹¤ê³  í•¨

## 2.3 Multi-Token Prediction

- Next Tokenë§Œ ì˜ˆì¸¡í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼ (ë©”ë‘ì‚¬ ì²˜ëŸ¼) **multiple future token**ì„ ì˜ˆì¸¡

  ![](../images/2025-02-01/image-20250131155953827.png)

  - Independent layer

    - projection matrix $M_k \in \mathbb{R}^{d \times 2d}$

      - ì´ì „ (k-1) depthì˜ representation **h**$_i^{k-1} \in \mathbb{R}^d$ê³¼, (i+k) token $Emb(t_{i+k}) \in \mathbb{R}^d$ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ projection $M_k$ë¥¼ í†µê³¼ì‹œí‚´

        ![](../images/2025-02-01/image-20250131160509571.png)

      - Transformer block ($TRM_k$)

        - projection matrix í†µê³¼í•œ ê²°ê³¼ **h'**$_i^{k-1} \in \mathbb{R}^d$ë¥¼ ì…ë ¥ë°›ì•„ Transformer blockì„ í†µê³¼ì‹œí‚´

          ![](../images/2025-02-01/image-20250131160636908.png)

  - Shared layer

    - output head embedding

      - Independent layer í†µê³¼ëœ ê°’ **h**$_{1:T-k}^k \in \mathbb{R}^d$ì„ ì…ë ¥ë°›ì•„, prediction token **P**$_{i+k+1}^{k} \in \mathbb{R}^V$ë¥¼ ì˜ˆì¸¡

        ![](../images/2025-02-01/image-20250131160844697.png)

    - input embedding

- MTP Training Objective

  ![](../images/2025-02-01/image-20250131161004320.png)

  ![](../images/2025-02-01/image-20250131161036565.png)

# 3. Infrastructures

## 3.1 Compute Cluster

- H800 x 2,048ê°œë¡œ í•™ìŠµ
  - 8ê°œì”© NVLink + NVSwitch + InfiniBand(IB)ë¡œ nodeê°„ ì—°ê²°

## 3.2 Training Framework

- HAI-LLM framework

  - 16-way Pipeline Parallelism (PP) $\to$ DualPipeline algorithmìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”ë¥¼ ìˆ˜í–‰í•¨
  - 64-way Expert Parallelism (EP) (@8 nodes)
  - ZeRO-1 Data Parallelism (DP)

- DualPipeì™€ ê³„ì‚°-í†µì‹  ì˜¤ë²„ë©

  - **DualPipe ê°œë…**:

    - **ê³„ì‚°**ê³¼ **í†µì‹ **ì„ **Forward** ë° **Backward** chunk ë‚´ì—ì„œ **ìƒí˜¸ ì˜¤ë²„ë©í•˜ì—¬ íŒŒì´í”„ë¼ì¸ ë²„ë¸”ì„ ê°ì†Œ**.

    - **ì²­í¬ ë¶„í• **: attention, all-to-all dispatch, MLP, all-to-all combineìœ¼ë¡œ ì„¸ë¶„í™”.

    - í¬ì›Œë“œ ë° ë°±ì›Œë“œ ì²­í¬ ë‚´ì˜ ì„¸ë¶€ ìš”ì†Œ(input, weight)ë¥¼ ì¬êµ¬ì„±í•˜ì—¬ GPU ì‚¬ìš© íš¨ìœ¨ ê·¹ëŒ€í™”.

      ![](../images/2025-02-01/image-20250131161724753.png)

  - **íš¨ìœ¨ì„±**:

    - íŒŒì´í”„ë¼ì¸ ì–‘ ë°©í–¥ì—ì„œ **ë¯¸ì„¸ ë°°ì¹˜ ë‹¨ìœ„ì˜ ì˜¤ë²„ë©**.
    - **ZeroBubble ë° 1F1B** ëŒ€ë¹„ í†µì‹  ì˜¤ë²„í—¤ë“œ ê°ì†Œ ë° ë²„ë¸” ìµœì†Œí™”.

  - **ë¹„êµ**:

    ![](../images/2025-02-01/image-20250131162019384.png)

    - DualPipe: íŒŒì´í”„ë¼ì¸ ë²„ë¸”ì´ **PP í¬ê¸° ì ˆë°˜ ìˆ˜ì¤€**ìœ¼ë¡œ ê°ì†Œ.
    - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì†Œí­ ì¦ê°€(2ë°°)í•˜ë‚˜ **ëª¨ë¸ ë³‘ë ¬ íš¨ìœ¨ì„± ì¦ê°€**.
    - **í™œì„± ë©”ëª¨ë¦¬ ì¦ê°€**: ì•½ 1/PP ë°°.

  - **ì£¼ìš” ê²°ê³¼**:

    - ê³„ì‚°-í†µì‹  ë¹„ìœ¨ ìœ ì§€í•˜ë©° ë†’ì€ í™•ì¥ì„±.
    - ëŒ€ê·œëª¨ íŒŒì´í”„ë¼ì¸ì—ì„œë„ **í†µì‹ -ê³„ì‚° ì˜¤ë²„ë©**ì„ í†µí•œ ì„±ëŠ¥ ê·¹ëŒ€í™”

- ë…¸ë“œ ê°„ All-to-All í†µì‹ ì˜ íš¨ìœ¨ì  êµ¬í˜„
  - DualPipe ì„±ëŠ¥ ìµœì í™”
    - ë…¸ë“œ ê°„ All-to-All í†µì‹  ì»¤ë„(ì‘ì—… ë¶„ë°°ì™€ ê²°í•© í¬í•¨)ì„ íš¨ìœ¨ì ìœ¼ë¡œ êµ¬í˜„í•˜ì—¬ SM (Streaming Multiprocessor) ì‚¬ìš©ëŸ‰ì„ ìµœì í™”
  - í´ëŸ¬ìŠ¤í„° ì„¤ê³„
    - ë…¸ë“œ ê°„ GPUëŠ” IB(InfiniBand)ë¡œ ì™„ì „ ì—°ê²°.
    - ë…¸ë“œ ë‚´ í†µì‹ ì€ NVLinkë¥¼ ì‚¬ìš©.
    - NVLink ëŒ€ì—­í­: 160GB/s (IB ëŒ€ì—­í­ì˜ ì•½ 3.2ë°°).
  - **í†µì‹  íš¨ìœ¨í™” ì „ëµ**
    - ê° í† í°ì€ ìµœëŒ€ 4ê°œì˜ ë…¸ë“œë¡œë§Œ ì „ë‹¬, IB íŠ¸ë˜í”½ ê°ì†Œ.
    - IBë¡œ ì „ë‹¬ëœ ë°ì´í„°ëŠ” NVLinkë¥¼ í†µí•´ ëª©í‘œ GPUë¡œ ì¦‰ì‹œ ì „ë‹¬.
    - IBì™€ NVLink ê°„ í†µì‹ ì€ ì™„ì „ ì¤‘ì²©(overlapping)ë˜ì–´ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”.
    - í‰ê· ì ìœ¼ë¡œ ê° ë…¸ë“œì—ì„œ 3.2ê°œì˜ ì „ë¬¸ê°€ë¥¼ ì„ íƒ ê°€ëŠ¥.
  - **ê²°ê³¼**
    - ì‹¤ì§ˆì ìœ¼ë¡œ 8ëª…ì˜ ì „ë¬¸ê°€ë¡œ êµ¬ì„±ë˜ì§€ë§Œ, ìµœëŒ€ 13ëª…ê¹Œì§€ í™•ì¥ ê°€ëŠ¥ (4 ë…¸ë“œ Ã— 3.2 ì „ë¬¸ê°€/ë…¸ë“œ).
    - 20ê°œì˜ SMë§Œìœ¼ë¡œ IBì™€ NVLink ëŒ€ì—­í­ì„ ì™„ì „íˆ í™œìš©.
  - **Warp ìµœì í™”**
    - 20 SMì„ 10ê°œì˜ í†µì‹  ì±„ë„ë¡œ ë¶„í• .
    - ë™ì ìœ¼ë¡œ ì¡°ì •ëœ Warpë¥¼ í†µí•´ IB ë° NVLink ì‘ì—… ì²˜ë¦¬:
      1. IB ì†¡ì‹ 
      2. IB-NVLink ì „ë‹¬
      3. NVLink ìˆ˜ì‹ 
    - ê²°í•© ê³¼ì •ì—ì„œë„ NVLink ì†¡ì‹  ë° IB ìˆ˜ì‹ ì´ Warpë¥¼ í†µí•´ ì¡°ì •.
  - **ì¶”ê°€ ìµœì í™”**
    - PTX(ë³‘ë ¬ ìŠ¤ë ˆë“œ ì‹¤í–‰) ëª…ë ¹ì–´ì™€ í†µì‹  ì²­í¬ í¬ê¸° ìë™ ì¡°ì • ì‚¬ìš©.
    - L2 ìºì‹œ ì‚¬ìš© ë° SM ê°„ ê°„ì„­ì„ ìµœì†Œí™”.
- ìµœì†Œí•œì˜ Overheadë¡œ Memoryì˜ ë¹„ì•½ì  ì ˆê° íŒ
  - MLA Up-Projectionê³¼ RMSNormì˜ ì¬ê³„ì‚°
    - backpropagation ê³¼ì •ì—ì„œ ì¬ê³„ì‚°ì„ ìˆ˜í–‰í•¨ìœ¼ë¡œì¨, memoryì— output activationì„ ì €ì¥í•˜ì§€ ì•Šë„ë¡ í•¨ $\to$ memoryë¥¼ ë¹„ì•½ì ìœ¼ë¡œ ì ˆê°
  - EMA in CPU
    - ë§¤ training stepë§ˆë‹¤ EMAë¥¼ ìœ„í•œ parameterë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ CPUì—ì„œ ê³„ì‚°í•˜ê²Œ í•¨
  - Multi-Token predictionì—ì„œ Shared embedding & output head
    - DualPipeì—ì„œ ë™ì¼ PP rankì— ë‘ layerë¥¼ í• ë‹¹í•˜ì—¬, ë©”ëª¨ë¦¬ë¥¼ ì¶”ê°€ í–¥ìƒì‹œí‚´

## 3.3 FP8ë¡œ í•™ìŠµ

### Mixed Precision Framework

![](../images/2025-02-01/image-20250131165119502.png)

- í•µì‹¬ computation kernel, GEMM operationsì˜ inputì„ FP8 precision, outputì„ BF16 or FP32ë¡œ ë‘ 

  - Activation: FP8
    - forward pass, weight backward pass, activation backward pass

  - Optimizer state: BF16
    - embedding module, output head, MoE gating modules, normalization operators, attention operators

  $\to$ ìƒëŒ€ì  ì—ëŸ¬ê°€ FP16 ëŒ€ë¹„ 0.25%ë¥¼ ë‚˜íƒ€ëƒ„

  ![](../images/2025-02-01/image-20250131165216742.png)

### Quantization & Multiplicationì˜ Precision í–¥ìƒí•˜ëŠ” ë°©ë²•

![](../images/2025-02-01/image-20250131171637543.png)

- Fine-grained quantization ì „ëµì„ ì œì•ˆí•¨

  - tile-wise grouping ($1 \times N_c$ elements)

    - **ì…ë ¥ ë°ì´í„°(í™œì„±í™” ê°’)**ë¥¼ 1x128ì˜ ì‘ì€ íƒ€ì¼(ì‘ì€ ë°ì´í„° ë¬¶ìŒ)ë¡œ ë‚˜ëˆ„ê³ , ê° íƒ€ì¼ë§ˆë‹¤ â€œìŠ¤ì¼€ì¼ë§ íŒ©í„°â€ë¥¼ ì ìš©í•¨.
    - ì¦‰, ë¬¶ìŒë³„ë¡œ í¬ê¸°ë¥¼ ë‹¤ë¥´ê²Œ ì¡°ì •í•´ ë” ì„¸ë°€í•˜ê²Œ ë‹¤ë£¸

  - block-wise grouping ($N_c \times N_c$ elements)

    - **ê°€ì¤‘ì¹˜ ë°ì´í„°**ë¥¼ 128x128 ë¸”ë¡ ë‹¨ìœ„ë¡œ ë‚˜ëˆ ì„œ ìŠ¤ì¼€ì¼ë§í•¨

    $\to$ ê° ë¸”ë¡ì´ ë°ì´í„° íŠ¹ì„±ì— ë§ê²Œ ì¡°ì •ë¨

- Increasing Accumulation Precision

  1. **ë¬¸ì œì **  
     - FP8 ì—°ì‚°ì—ì„œ ëˆ„ì  ì •í™•ë„(Accumulation Precision)ê°€ ë‚®ì•„ì§€ë©°, íŠ¹íˆ í° ì°¨ì›(K=4096)ì—ì„œëŠ” ìµœëŒ€ 2% ì˜¤ì°¨ ë°œìƒ.  
     - FP32ì™€ ë¹„êµí•´ FP8 ëˆ„ì ì€ ì•½ 14ë¹„íŠ¸ì˜ ì œí•œëœ ì •í™•ë„ë¥¼ ê°€ì§.
  2. **í•´ê²°ì±…: CUDA Core**
     - **Tensor Core**ì—ì„œ FP8 ëˆ„ì  ê²°ê³¼ë¥¼ ì¼ì • ê°„ê²©(ğ‘ğ¶=128)ë§ˆë‹¤ **FP32 ë ˆì§€ìŠ¤í„°**ë¡œ ë³µì‚¬í•˜ì—¬ ê³ ì •ë°€ ëˆ„ì  ìˆ˜í–‰.  
  3. **íš¨ìœ¨ì„± ìœ ì§€**  
     - ë‘ WGMMA(Warpgroup-level Matrix Multiply-Accumulate) ì‘ì—… ë³‘ë ¬ ì‹¤í–‰ìœ¼ë¡œ Tensor Core ì‚¬ìš©ë¥  ìœ ì§€.  
     - ğ‘ğ¶=128(4 WGMMAs) ì„¤ì •ì´ ìµœì†Œ ì˜¤ë²„í—¤ë“œë¡œ ì •ë°€ë„ë¥¼ í¬ê²Œ í–¥ìƒ.
  4. **ê²°ê³¼**  
     - ë‚®ì€ ëˆ„ì  ì •ë°€ë„ ë¬¸ì œ í•´ê²° ë° í›ˆë ¨ ì •í™•ë„ ê°œì„ .

- Mantissa over Exponents
  - ê¸°ì¡´ Mixed FP8 frameworkëŠ” Fpropì—ëŠ” E4M3(4-bit exponent, 3-bit mantissa), Dgrad & Wgradì—” E5M2 (5-bit exponent, 2-bit mantissa)ë¥¼ ì‚¬ìš©
  - ìš°ë¦¬ëŠ” ì „ì²´ì— E4M3(4-bit exponent, 3-bit mantissa)ë¥¼ ì‚¬ìš©
- Online Quantization
  - ì‹¤ì‹œê°„ìœ¼ë¡œ Maximum absolute valueë¥¼ ê³„ì‚°í•˜ëŠ” "Delayed quantization"ì„ ì°¨ìš©í•˜ì—¬ Scaleê°’ì„ onlineìœ¼ë¡œ ê³„ì‚°í•˜ì—¬ fine-grained quationzationì„ ìˆ˜í–‰

### Low-precision Storage & Communication

- Low Precision Optimizer States

  - AdamW ì˜µí‹°ë§ˆì´ì €ì˜ 1ì°¨Â·2ì°¨ ëª¨ë©˜íŠ¸ â†’ FP32 ëŒ€ì‹  BF16 ì‚¬ìš© (ì„±ëŠ¥ ì €í•˜ ì—†ìŒ)
  - ë§ˆìŠ¤í„° ê°€ì¤‘ì¹˜ ë° ê·¸ë˜ë””ì–¸íŠ¸ â†’ FP32 ìœ ì§€ (ìˆ˜ì¹˜ì  ì•ˆì •ì„± ë³´ì¥)

- Low Precision Activation

  - **Wgrad ì—°ì‚°ì„ FP8ë¡œ ìˆ˜í–‰**í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½

    - Linear ì—°ì‚°ìì˜ ì—­ì „íŒŒ í™œì„±í™” ìºì‹± â†’ FP8 í™œìš©

      1. Attention ì—°ì‚° í›„ Linear ì…ë ¥ê°’
         - ì—­ì „íŒŒ ì‹œ ì¤‘ìš”í•˜ë¯€ë¡œ **E5M6 ë°ì´í„° í˜•ì‹ ì‚¬ìš©**
         - 1x128 íƒ€ì¼ â†’ 128x1 íƒ€ì¼ë¡œ
         - **ìŠ¤ì¼€ì¼ë§ ê³„ìˆ˜ = 2ì˜ ê±°ë“­ì œê³±**(ì–‘ìí™” ì˜¤ë¥˜ ë°©ì§€)

      	2. MoEì˜ SwiGLU ì—°ì‚°ì ì…ë ¥ê°’
          - ì…ë ¥ê°’ì„ **FP8ë¡œ ì €ì¥ í›„, ì—­ì „íŒŒ ì‹œ ì¶œë ¥ê°’ ì¬ê³„ì‚°**
          - ë©”ëª¨ë¦¬ ì ˆì•½ê³¼ ì—°ì‚° ì •í™•ì„± ê· í˜• ìœ ì§€

  - Low Precision Communication

    - MoE Up-projection ì „ í™œì„±í™” ê°’ FP8ë¡œ ì–‘ìí™” â†’ FP8 Fpropê³¼ í˜¸í™˜
    - **ìŠ¤ì¼€ì¼ë§ ê³„ìˆ˜ = 2ì˜ ê±°ë“­ì œê³±**, Linear ì—°ì‚° í›„ attention ì…ë ¥ê³¼ ë™ì¼
    - MoE down-projection ì „ í™œì„±í™” ê·¸ë˜ë””ì–¸íŠ¸ â†’ FP8 ìœ ì§€
    - Forward ë° backward ê²°í•© ì—°ì‚°ì€ BF16 ìœ ì§€ â†’ í›ˆë ¨ ì •í™•ë„ ë³´ì¥

## 3.4 Inference & Deployment

- ê¸°ë°˜ ì¸í”„ë¼
  - H800 í´ëŸ¬ìŠ¤í„°ì—ì„œ NVLink(ë…¸ë“œ ë‚´) ë° IB(ë…¸ë“œ ê°„) ì‚¬ìš©
  - Prefilling(ì‚¬ì „ ì±„ìš°ê¸°)ê³¼ Decoding(ë””ì½”ë”©) ë‹¨ê³„ë¥¼ ë¶„ë¦¬í•˜ì—¬ ì„±ëŠ¥ ìµœì í™”

### Prefilling

- 4ê°œ ë…¸ë“œ, 32ê°œ GPUê°€ ìµœì†Œ ë°°í¬ ë‹¨ìœ„
- TP4 (4-way Tensor Parallelism) + SP (Sequence Parallelism) + DP8 (8-way Data Parallelism) ì¡°í•©
- MoE: EP32(32-way Expert Parallelism) â†’ ëŒ€ê·œëª¨ ë°°ì¹˜ í¬ê¸° ì²˜ë¦¬ ê°€ëŠ¥
- ê· í˜• ì¡íŒ MoE ì „ë¬¸ê°€ ë¡œë“œ ë°¸ëŸ°ì‹±
- ë¶€í•˜ê°€ í° ì „ë¬¸ê°€ë¥¼ **ì¤‘ë³µ ë°°ì¹˜**í•˜ì—¬ GPU ê°„ ë¶€í•˜ ê· í˜• ìœ ì§€
  - 10ë¶„ë§ˆë‹¤ ì „ë¬¸ê°€ ë¶€í•˜ í†µê³„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¡°ì •
  - ê° GPU: ì›ë˜ 8ê°œì˜ ì „ë¬¸ê°€ + 1ê°œì˜ ì¤‘ë³µ ì „ë¬¸ê°€
- ë§ˆì´í¬ë¡œ ë°°ì¹˜ ìµœì í™”
  - 2ê°œì˜ ë§ˆì´í¬ë¡œ ë°°ì¹˜ë¥¼ ë™ì‹œì— ì²˜ë¦¬ â†’ í†µì‹ ê³¼ ì—°ì‚°ì„ ê²¹ì³ ì‹¤í–‰
  - ë™ì  ì¤‘ë³µ ì „ëµ ì‹¤í—˜ ì¤‘
    - ê° GPUê°€ 16ê°œ ì „ë¬¸ê°€ë¥¼ í˜¸ìŠ¤íŒ…í•˜ì§€ë§Œ, í•œ ë²ˆì— 9ê°œë§Œ í™œì„±í™”
    - ë ˆì´ì–´ë³„ë¡œ ìµœì ì˜ ë¼ìš°íŒ… ë°©ì‹ ì‹¤ì‹œê°„ ê³„ì‚° â†’ ê³„ì‚° ë¶€ë‹´ ì—†ìŒ

### Decoding

- 40ê°œ ë…¸ë“œ, 320ê°œ GPUê°€ ìµœì†Œ ë°°í¬ ë‹¨ìœ„
- TP4 + SP (Attention ì—°ì‚°) + DP80 ì¡°í•©
- MoE: EP320 (ê° GPUê°€ 1ê°œ ì „ë¬¸ê°€ ì „ë‹´, 64ê°œ GPUëŠ” ì¤‘ë³µÂ·ê³µìœ  ì „ë¬¸ê°€ í˜¸ìŠ¤íŒ…)
- All-to-All í†µì‹  â†’ IB í¬ì¸íŠ¸ íˆ¬ í¬ì¸íŠ¸ ì „ì†¡(ë‚®ì€ ì§€ì—°ì‹œê°„ ìœ ì§€)
- IBGDA(NVIDIA, 2022) ê¸°ìˆ  í™œìš© â†’ ì§€ì—°ì‹œê°„ ìµœì†Œí™”
- ì „ë¬¸ê°€ ë¶€í•˜ë¥¼ ì¼ì • ì£¼ê¸°ë¡œ ë¶„ì„í•˜ì—¬ ì¤‘ë³µ ì „ë¬¸ê°€ ì„¤ì • ì¡°ì •
- ë§ˆì´í¬ë¡œ ë°°ì¹˜ ìµœì í™”
  - Attentionì´ ë§ì€ ì—°ì‚°ì„ ì°¨ì§€í•˜ë¯€ë¡œ, **ì²« ë²ˆì§¸ ë§ˆì´í¬ë¡œ ë°°ì¹˜ì˜ Attentionì„ ì‹¤í–‰í•˜ëŠ” ë™ì•ˆ ë‘ ë²ˆì§¸ ë°°ì¹˜ì˜ MoE(ë””ìŠ¤íŒ¨ì¹˜+ì»´ë°”ì¸)ë¥¼ ë³‘ë ¬ ì²˜ë¦¬**
- ë””ì½”ë”© ë‹¨ê³„ì˜ ë³‘ëª© = ë©”ëª¨ë¦¬ ì ‘ê·¼ ì†ë„
  - MoE ì—°ì‚° ì‹œ ì ì€ SMì„ í• ë‹¹í•˜ì—¬ Attention ì—°ì‚° ì†ë„ë¥¼ ìµœì í™”

## 3.5 í•˜ë“œì›¨ì–´ ì„¤ê³„ ì œì•ˆ

###  í†µì‹  í•˜ë“œì›¨ì–´ (Communication Hardware)

- í˜„ì¬ ë¬¸ì œ:  í†µì‹  ì‘ì—…ì´ SM(ìŠ¤íŠ¸ë¦¬ë° ë©€í‹°í”„ë¡œì„¸ì„œ)ì—ì„œ ì‹¤í–‰ë˜ì–´ ì—°ì‚° ìì› ë‚­ë¹„
- ì œì•ˆ:
  - í†µì‹  ì‘ì—…ì„ NVIDIA SHARP ê°™ì€ ì „ìš© GPU/ë„¤íŠ¸ì›Œí¬ ê³µë™ í”„ë¡œì„¸ì„œì—ì„œ ì²˜ë¦¬
  - IB(ìŠ¤ì¼€ì¼ì•„ì›ƒ)ì™€ NVLink(ìŠ¤ì¼€ì¼ì—…) ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•© â†’ í”„ë¡œê·¸ë˜ë° ë‹¨ìˆœí™” ë° ì„±ëŠ¥ í–¥ìƒ

### ì—°ì‚° í•˜ë“œì›¨ì–´ (Compute Hardware)

- FP8 GEMM ëˆ„ì  ì—°ì‚° ì •ë°€ë„ í–¥ìƒ
  - í˜„ì¬ **FP8 GEMM ëˆ„ì  ì—°ì‚° ì‹œ 14ë¹„íŠ¸ ì •ë°€ë„ë§Œ ìœ ì§€** â†’ FP32 ë³€í™˜ ì‹œ ì˜¤ë¥˜ ë°œìƒ
  - ìµœì†Œ 34ë¹„íŠ¸ ëˆ„ì  ì •ë°€ë„ í•„ìš” â†’ í›ˆë ¨ ë° ì¶”ë¡  ì‹œ ì •ë°€ë„ í–¥ìƒ ê°€ëŠ¥
- íƒ€ì¼Â·ë¸”ë¡ ë‹¨ìœ„ ì–‘ìí™” ì§€ì›
  - í˜„ì¬ GPUëŠ” per-tensor ì–‘ìí™”ë§Œ ì§€ì›
  - ì œì•ˆ:
    - Tensor Coreì—ì„œ íƒ€ì¼Â·ë¸”ë¡ ë‹¨ìœ„ ì–‘ìí™”ë¥¼ ì§ì ‘ ì§€ì›í•˜ì—¬ CUDA ì½”ì–´ ê°„ ë¶ˆí•„ìš”í•œ ë°ì´í„° ì´ë™ ìµœì†Œí™”

- ì˜¨ë¼ì¸ ì–‘ìí™” ì§€ì›
  - **í˜„ì¬ ë¬¸ì œ**: FP8 ë³€í™˜ì„ ìœ„í•´ HBM(ê³ ëŒ€ì—­í­ ë©”ëª¨ë¦¬)ì—ì„œ ì—¬ëŸ¬ ë²ˆ ë°ì´í„°ë¥¼ ì½ê³  ì”€
  - ì œì•ˆ:
    - FP8 ë³€í™˜ê³¼ TMA(Tensor Memory Accelerator) ì ‘ê·¼ì„ í†µí•©í•˜ì—¬ ë©”ëª¨ë¦¬ ì „ì†¡ ì¤‘ ì–‘ìí™” ìˆ˜í–‰
    - ì›Œí”„ ìˆ˜ì¤€ ìºìŠ¤íŠ¸ ëª…ë ¹ì–´ ì¶”ê°€ â†’ Layer Normalizationê³¼ FP8 ë³€í™˜ì„ íš¨ê³¼ì ìœ¼ë¡œ ê²°í•©
    - Near-Memory Computing ì±„íƒ â†’ 
      - BF16 ë°ì´í„°ë¥¼ FP8ë¡œ ë³€í™˜í•˜ëŠ” ì‘ì—…ì„ HBMì—ì„œ ì½ì–´ì˜¤ëŠ” ìˆœê°„ ìˆ˜í–‰
      - ì˜¤í”„ì¹© ë©”ëª¨ë¦¬ ì ‘ê·¼ëŸ‰ 50% ì ˆê° ê°€ëŠ¥
- Transposed GEMM ì—°ì‚° ì§€ì›
  - **í˜„ì¬ ë¬¸ì œ**: ì „ì¹˜(transpose) ì—°ì‚°ì„ ìœ„í•´ ì¶”ê°€ì ì¸ ë©”ëª¨ë¦¬ ì‘ì—… í•„ìš”
  - ì œì•ˆ:
    - ê³µìœ  ë©”ëª¨ë¦¬ì—ì„œ ì§ì ‘ ì „ì¹˜ëœ ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ì½ëŠ” ê¸°ëŠ¥ ì¶”ê°€
    - FP8 ë³€í™˜ ë° TMA ì ‘ê·¼ ìµœì í™”ì™€ ê²°í•©í•˜ì—¬ ì„±ëŠ¥ ê°œì„ 

# 4. Pretraining

## 4.1 Data Construction

- DeepSeekV2 ì±„ìš©

  - Fill-in-Middle (FIM) ì „ëµ

    - Next-token Prediction ì‹œ, ì¤‘ê°„ textë¥¼ ì˜ˆì¸¡í•˜ëŠ” í•™ìŠµë°©ë²•

      ![](../images/2025-02-01/image-20250131185412114.png)

- DeepSeekV2 ëŒ€ë¹„ ë³€ê²½ì‚¬í•­

  - ë‹¤êµ­ì–´ ì§€ì›
  - ìˆ˜í•™ & í”„ë¡œê·¸ë˜ë° ë°ì´í„° ì¶”ê°€ í•™ìŠµ $\to$ 14.8T ê³ í’ˆì§ˆ ë‹¤ì–‘í•œ tokens 
  - 128Kì˜ vocabularyë¡œ êµ¬ì„±ëœ tokenizer ì‚¬ìš©
  - token bias ì œê±°ë¥¼ ìœ„í•´, line breakê°€ ì—†ëŠ” lineì˜ ê²½ìš°, multi-line ì´ ë˜ë„ë¡ randomí•˜ê²Œ line-breaking

## 4.2 Hyperparameter

- layer ìˆ˜: 61
- hidden dimension: 7,168
- per-head dimension $d_h$: 128
- number of attention heads $n_h$: 128
- key-value compression dimension $d_c$: 512
- query compression dimension $d'_c$: 1,536
- Rotation embedding per-head dimension $d_h^R$: 64
- shared expert ìˆ˜ $n_s$: 1
- routed expert ìˆ˜ $n_r$: 256
- intermediate hidden dimension **h**: 2,048
- Multi token prediction depth *D*: 1
- DeepSeek-V3: 671B (37B)

### Training Hyperparameter

- Maximum sequence length: 4K
- Batch size scheduling: ì²˜ìŒ (3,072) $\to$ ë‚˜ì¤‘ (15,360)ë¡œ batch sizeë¥¼ ë°”ê¿ˆ

## 4.3 Long Context Extension

- YaRN: Max Contextë¥¼ 4K $\to$ 32K, 32K $\to$ 128Kë¡œ ëŠ˜ë¦¬ë©° í•™ìŠµ

  ![](../images/2025-02-01/image-20250131191136765.png)

## 4.4 Evaluation

- ë°ì´í„°ì…‹

  - English, MultiLingual, Math, Codeìœ„ì£¼ì˜ benchmark

- ì •ëŸ‰ì  ê²°ê³¼ (vs. OpenSource Models)

  ![](../images/2025-02-01/image-20250131191319349.png)

- Ablation study

  - With / WIthout MTP

    ![](../images/2025-02-01/image-20250131192322568.png)

  - With / Without Auxiliary-Loss-Free Balancing

    ![](../images/2025-02-01/image-20250131192404233.png)

- Batch-wise Load Balance vs. Sequence-wise Load Balance

  - ê²°ë¡ : Batch-wise Load Balance (Auxiliary-loss-free) ê°€ ë” ì¢‹ë‹¤.

    ![](../images/2025-02-01/image-20250131192827054.png)

    

# 5. Post-Training

## 5.1 Supervised Finetuning

- Reasoning Data
  - ìˆ˜í•™, ì½”ë”© ëŒ€íšŒ, Logic í¼ì¦ ë“±ìœ¼ë¡œ êµ¬ì„±ëœ ë°ì´í„°ì…‹
  - ë°ì´í„° íšë“ ë°©ë²•
    - DeepSeek-R1ì„ ëª¨ë°©í•œ domain specific expertë¥¼ Reinforcement Learningìœ¼ë¡œ í•™ìŠµ
      - Reflection & Verficationì„ ìˆ˜í–‰í•˜ê²Œ í•˜ëŠ” system promptë¥¼ ë„ì…í•˜ì—¬ RLë¡œ í•™ìŠµ
      - <problem, original response>ì™€ <system prompt, problem, original response>ê°„ì˜ RL trainingìœ¼ë¡œ í•™ìŠµ
      - Rejection Samplingì„ í†µí•´ ì½”í’ˆì§ˆ (CoTìš©) ë°ì´í„°ì…‹ì„ ìƒì„±
- Non-Reasoning Data
  - DeepSeek-V2.5ë¥¼ í†µí•´ creative writing, role-play, simple QAë¥¼ ìƒì„±í•˜ê³ , human annotatorì—ê²Œ ê²€í† ì‹œì¼œ í•™ìŠµìš© ë°ì´í„°ì…‹ ì·¨ë“
- SFT Settings
  - 2-epoch í•™ìŠµ

## 5.2 Reinforcement Learning

### Reward Model

- Rule-based Reward Modelì„ í™œìš©
  - LeetCodeì˜ test-codeë¥¼ í™œìš©
  - Mathì˜ ì •ë‹µì´ box ë‚´ì— ìœ„ì¹˜í•´ì•¼í•¨ì„ í™œìš©
- Model-based Reward Modelì„ í™œìš©
  - Preference dataë¥¼ ì·¨ë“í•˜ì—¬ í•™ìŠµ
  - DeepSeek-V3 SFT checkpointë¡œë¶€í„° ì‹œì‘
  - CoTë„ í¬í•¨ë˜ë„ë¡ í•¨

### Group Relative Policy Optimization

- InterVL-2.5-DPOì™€ ë™ì¼

![](../images/2025-02-01/image-20250131194157739.png)

![](../images/2025-02-01/image-20250131194258166.png)

## 5.3 Evaluations

- ì •ëŸ‰ì  ê²°ê³¼ (vs. SOTA)

  ![](../images/2025-02-01/image-20250131194333057.png)

- English Open-ended conversation ê²°ê³¼

  ![](../images/2025-02-01/image-20250131194410442.png)

- LLM as a judge ëŠ¥ë ¥ ê²°ê³¼ (Generative Reward Model)

  ![](../images/2025-02-01/image-20250131194534696.png)

- DeepSeek R1ì˜ Distillaition í›ˆë ¨ ê²°ê³¼

  - Baseline: DeepSeek-V2.5ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Short CoT í•™ìŠµ

  - Distill: DeepSeek-V2.5ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìœ„ì— ì„¤ëª…í•œ expert modelì´ ìƒì„±í•œ distillation dataë¡œ í•™ìŠµ

    $\to$ distillation dataê°€ ë§¤ìš° íš¨ê³¼ì ì„ì„ ì…ì¦

  ![](../images/2025-02-01/image-20250131194649570.png)

### Multi-Token Prediction Evaluation

- Next 2 tokenì˜ˆì¸¡ + Speculative Decoding ë„ì… $\to$ decoding ì†ë„ 1.8ë°° í–¥ìƒ
