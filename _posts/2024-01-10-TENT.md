---
title: "[TTA][CLS] Tent: Fully test-time adaptation by entropy minimization"
---
# [TTA][CLS] Tent: Fully test-time adaptation by entropy minimization

- paper : https://arxiv.org/pdf/2006.10726.pdf

- git: https://github.com/DequanWang/tent

- Contribution

  - 배포된 모델의 상황에서 target dataset에만 접근 가능할 때, target dataset에서의 test-entropy를 최소화하는 방향으로 Test Time Adaptation (TTA)하는 setting을 제안함 → **TENT**
  - 구체적으로, Test환경에서 dataset의 통계 분포(평균, 표준편차)를 계산하고, normalization을 수행하며, test-entropy를 기반으로 scale & bias에 대한 gradient를 학습시켜 준다. (BN과 동일)
  - Source-free Domain Adaptation 에서 classification, semantic segmentation task에서 (당시) SOTA를 찍었다.

- Domain Shift

  - 학습 데이터 (Source data)로 학습한 모델이 배포된 시점에서 평가 데이터 (Target data)가 domain shift가 발생했을 경우, entropy는 domain shift를 나타내는 척도가 될 수 있음

    ![](../images/2024-01-09/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-01-10%2019-34-40.png)

- Setting : Fully Test-Time Adaptation

  ![](../images/2024-01-09/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-01-10%2019-35-01.png)

  - vs. fine tuning & domain adaptation & test-time training
    - Fine-tuning
      - Target domain : supervised loss
    - domain adaptation
      - Source domain : supervised loss 
      - Target domain : unsupervised loss
    - Tet-time training : 
      - Source domain  : supervised loss + self-supervised loss
      - Target domain : self-supervised loss (a.k.a unsupervised loss)
    - Tent : 
      - Target domain : Entropy loss (a.k.a unsupervised loss)

- TENT

  ![](../images/2024-01-09/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-01-10%2019-35-21.png)

  ![](../images/2024-01-09/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-01-10%2019-35-40.png)

  - test-time 에서 non-linear한 f를 전부 adaptation하면 too sensitive해진다고 함.

    - non-linear f 특성 중, linear 한 특성 (scale, bias)만 학습함

  - test-time 에서 parameter theta를 전부 adaptation하면 too sensitive해진다고 함.

    - channel-wise dim으로 scale&bias값을 transformation을 수행함

    - Algorithm

      - Initialization

        ![](../images/2024-01-09/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-01-10%2019-35-56.png)

        - l번째 layer, k번째 channel에 대한 scale, bias 값을 제외한 parameter는 fix

      - Iteration

        - batch 단위로 normalization 통계 분포와 transformation parameter를 업데이트
        - transformation 값 (scale, bias)는 Entropy값을 근거로 back-propagation으로 계산 
          - next batch에만 영향을 줌

      - Termination

        - Online은 Termination이 필수가 아님
        - Offline은 adaptation 후 inference를 반
