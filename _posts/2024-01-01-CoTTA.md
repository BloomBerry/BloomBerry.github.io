---
title: "[TTA][CLS][SS] Continual Test-Time Domain Adaptation"
---
# [TTA][CLS][SS] Continual Test-Time Domain Adaptation

- paper : https://arxiv.org/pdf/2203.13591.pdf

- git : https://qin.ee/cotta/

- CVPR 2022 (인용수 : 101회, '22.03)

- downstream task : Continual Domain Adaptation for Classification & Semantic Segmentation

- Contribution

  - Target domain이 continually changing environment라 가정하는 “continual domain adaptation”을 제안함

    1. weighted-averaging (EMA) & augmentation-averaged prediction (ensemble) 사용하여 pseudo label의 quality를 향상시킴

       ![](../images/2024-01-01/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-01-01%2023-50-42.png)

    2. stochastically network weight의 일부를 fixed-source의 weight로 대체시킴으로 catastrophic forgetting 방지 → 기존 (TENT)엔 BN의 weight만 학습했으므로, 학습 가능한 capacity 가 증가함

- Continual Test-Time Domain Adaptation

  ![](../images/2024-01-01/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-01-01%2023-51-35.png)

  - online continual test-time adaptation
    - related works
      - domain adaptation
      - test-time adaptation
      - domain generalization
      - continual learning

- Methodology

  - Weighted Averaged Pseudo Labels

    ![](../images/2024-01-01/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-01-01%2023-52-09.png)

    - EMA teacher 의 soft-label을 pseudo label로 사용함

      ![](../images/2024-01-01/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-01-01%2023-52-31.png)

  - Augmentation-Averaged Pseudo-Labels

    ![](../images/2024-01-01/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-01-01%2023-52-49.png)

    - 가정 : Source-trained Fixed Teacher weight가 있음

    - 가정 2: Target domain과 source domain간의 gap이 클수록, confidence score가 낮아짐

    - Domain Gap이 클 경우엔 서로 다른 N개의 augmentation을 수행하여 평균 vector를 pseudo label로 활용함

      ![](../images/2024-01-01/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-01-01%2023-53-18.png)

      - $p_th$ : confidence threshold
      - $conf(f_{\theta_0})$ : source-fixed model의 confidence score

- Stochastic Restoration

  - 장기간 adaptation할 경우, source의 정보를 catastrophic forgetting할 수 있음 

    ![](../images/2024-01-01/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-01-01%2023-54-06.png)

  - 모델의 weight를 random하게 source의 weight로 대치시킴 → Drop-out효과 (Regulaization)

    ![](../images/2024-01-01/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-01-01%2023-54-21.png)

- Overall Algorithm

  ![](../images/2024-01-01/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-01-01%2023-54-35.png)

- Overall Sturcture

  ![](../images/2024-01-01/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-01-01%2023-54-58.png)
