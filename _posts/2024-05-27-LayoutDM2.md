---
title: "[LG] LayoutDM: Transformer-based Diffusion Model for Layout Generation"
---
# [LG] LayoutDM: Transformer-based Diffusion Model for Layout Generation

- paper: https://arxiv.org/pdf/2305.02567
- github: X
- CVPR 2023 accepted (인용수: 22회, '24-05-27 기준)
- Downstream task: Layout Generation

# 1. Motivation

- Diffusion model의 근래 성공에 영감을 얻어 high-quality layout을 Transformer 기반의 encoder로 생성해보면 좋을 것 같다는 생각이 듬

# 2. Contribution

- User-specified attribute기반의 주어진 element를 Transformer + Diffusion 기반으로 layout을 생성하는 LayoutDM을 제안
  - high-quality generation, better diversity, faithful distribution coverage, stationary training등 Diffusion Model의 특성을 반영함
- Backbone을 U-Net에서 Transformer로 바꾼 cLayoutDenoiser를 제안함
- 5가지 benchmark에서 SOTA

# 3. LayoutDM

- Overall Architecture

  ![](../images/2024-05-27/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-05-27%2023-17-52.png)

- Preliminaries

  - Layout generation

    ![](../images/2024-05-27/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-05-27%2023-20-05.png)

    - *N*개의 layout element
    - $g_1$: 1번째 layout geometry (*x,y,w,h*)
    - $f_1$: 1번째 layout attribute

  - Forward process

    ![](../images/2024-05-27/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-05-27%2023-21-35.png)

  - Reverse process

    ![](../images/2024-05-27/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-05-27%2023-21-57.png)

  - Reparameterized

    ![](../images/2024-05-27/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-05-27%2023-22-08.png)

- Conditional Layout Denoiser

  - layout geometry **g**, layout attribute **f**와 time step *t*를 입력받아 noise $e_{\theta}(g_t, t, f)$ 를 예측

    ![](../images/2024-05-27/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-05-27%2023-24-00.png)

    - element order는 상관이 없으므로, positional encoding를 제외함
    - Geometric embedding은 4차원 (x,y,w,h)에서 더 의미 있는 embedding을 생성하기 위해 fixed length로 embedding 수행
    - TE: Sinusoidal time embedding 
    - Element Embedding: 주어진 hidden vectors $h_f, h_g$,TE(*t*)를 concat하여 FC통과 시켜 fusion 수행 

- Transformer Layer

  ![](../images/2024-05-27/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-05-27%2023-26-34.png)

- Training

  ![](../images/2024-05-27/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-05-27%2023-26-45.png)

  - Loss

    ![](../images/2024-05-27/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-05-27%2023-27-15.png)

- Inference

  ![](../images/2024-05-27/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-05-27%2023-27-02.png)

# 4. Experiments

- Quantitative Result

  ![](../images/2024-05-27/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-05-27%2023-27-35.png)

- Extended results for PubLayNet

  ![](../images/2024-05-27/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-05-27%2023-28-03.png)

- Qualitative Result

  ![](../images/2024-05-27/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-05-27%2023-29-07.png)

- Ablation 

  - Transformer Layer vs. FC Layer로 대체

    - Quantitative Result

      ![](../images/2024-05-27/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-05-27%2023-28-39.png)

    - Qualitative Result

      ![](../images/2024-05-27/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-05-27%2023-29-28.png)

    - Diversity Comparison

      ![](../images/2024-05-27/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-05-27%2023-30-05.png)

- Rendered Result

  ![](../images/2024-05-27/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-05-27%2023-30-26.png)

  ![](../images/2024-05-27/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202024-05-27%2023-31-03.png)
