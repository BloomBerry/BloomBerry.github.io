# [MobileUI] From Perception to Reasoning: Enhancing Vision-Language Models for Mobile UI Understanding

- paper: https://aclanthology.org/2025.findings-acl.1295.pdf
- github: https://github.com/iitbsrib/MoUI (깡통)
- ACL 2025 accepted (인용수: 0회, '25-07-29 기준)
- downstream task: Mobile UI grounding & QA

# 1. Motivation

- VLM에게 정밀하게 text & visual element를 grounding하는 것은 해당 요소를 인식하고, text query의 context를 고려하여 이해해야하는 어려움이 있다.

- 이는 Mobilie UI domain에서 domain specific한 structured UI dataset이 pretraining 시에 부족함 때문이다.

  ![](../images/2025-07-29/image-20250729222858353.png)

  $\to$ User의 의도와 Mobile UI의 시각적 의미간의 gap을 bridge하는 학습 방법을 제안해보자!

# 2. Contribution

- MoUI (Mobile UI) lightweight models을 제안함 (1B, 2B, 4B)
  - UI screens기반 복잡한 reasoning tasks에 사용됨
  - 2 stage training
    - perception: MoUI기반 학습
    - reasoning: Rico data기반 다양한 task
  - SoTA 성능을 달성함
- UI Instruct dataset(MoIT)을 제안함
  - Mobile UI grounding 기반 150K instruction-tuned dataset
  - **user의 queries**와 **UI element**간의 alignment
- MoIQ: 3K의 사람이 수동으로 라벨링한 reasoning evaluation benchmark

# 3. MoUI

# 4. Experiments